{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlqOxRiRFCAooOGGI41EbG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuguvi26/Copy-of-Enhanced-ETL-Workflow-with-Python-AWS-S3-RDS-and-Glue-for-Data-Engineers/blob/main/Copy_of_Enhanced_ETL_Workflow_with_Python%2C_AWS_S3%2C_RDS%2C_and_Glue_for_Data_Engineers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyN-oTtIxx-M",
        "outputId": "7e6e18e4-bf4d-4f58-ffdb-888cef51790e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ETL job starting\n",
            "Connected to S3 bucket: bhuguvibucket\n",
            "Pipeline started\n",
            "Listing objects under s3://bhuguvibucket/raw/\n",
            "Found 9 object(s) under prefix\n",
            "Extracting CSV: raw/source1.csv\n",
            "Extracted raw/source1.csv (rows=5)\n",
            "Extracting JSON (lines or array): raw/source1.json\n",
            "Extracted raw/source1.json (rows=4)\n",
            "Extracting XML: raw/source1.xml\n",
            "Extracted raw/source1.xml (rows=4)\n",
            "Extracting CSV: raw/source2.csv\n",
            "Extracted raw/source2.csv (rows=5)\n",
            "Extracting JSON (lines or array): raw/source2.json\n",
            "Extracted raw/source2.json (rows=4)\n",
            "Extracting XML: raw/source2.xml\n",
            "Extracted raw/source2.xml (rows=4)\n",
            "Extracting CSV: raw/source3.csv\n",
            "Extracted raw/source3.csv (rows=5)\n",
            "Extracting JSON (lines or array): raw/source3.json\n",
            "Extracted raw/source3.json (rows=4)\n",
            "Extracting XML: raw/source3.xml\n",
            "Extracted raw/source3.xml (rows=4)\n",
            "Combined dataframe rows: 39\n",
            "Normalizing columns and transforming units\n",
            "Transformation result rows: 39\n",
            "Uploading transformed CSV to s3://bhuguvibucket/transformed/transformed_data.csv\n",
            "Uploaded transformed CSV\n",
            "Ensuring database exists (attempt)\n",
            "Could not create DB (may lack privileges): Not an executable object: 'CREATE DATABASE IF NOT EXISTS `bhuguvidb`;'\n",
            "Loading dataframe to RDS table bhuguvidb.etl_merged_data\n",
            "Pushed dataframe to RDS table bhuguvidb.etl_merged_data\n",
            "Flushing log handlers before upload\n",
            "Uploading log to s3://bhuguvibucket/logs/etl_run_20251124T195645Z.log\n",
            "Uploaded log\n",
            "Pipeline completed successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed CSV: s3://bhuguvibucket/transformed/transformed_data.csv\n",
            "RDS table: bhuguvidb.etl_merged_data\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# ETL: process all files in s3://bhuguvibucket/raw/\n",
        "# Single Colab cell ‚Äî edit CONFIG before running\n",
        "# =========================\n",
        "\n",
        "# Install dependencies (first run)\n",
        "!pip install -q boto3 pandas sqlalchemy pymysql lxml\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from io import BytesIO, StringIO\n",
        "from datetime import datetime, timezone\n",
        "\n",
        "import boto3\n",
        "import botocore\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "from sqlalchemy import create_engine\n",
        "\n",
        "# -------------------------\n",
        "# CONFIG - Edit these values BEFORE running\n",
        "# -------------------------\n",
        "AWS_REGION = \"ap-southeast-2\"                   # exact region code\n",
        "AWS_ACCESS_KEY = \"AKIAXJ6ZHROTHSV54HF7\"\n",
        "AWS_SECRET_KEY = \"dtLptl2I6oXxGdYmGdN6vksWBvRkP8KLLliBjPoB\"  # <<< REPLACE locally in Colab\n",
        "\n",
        "S3_BUCKET = \"bhuguvibucket\"                     # your bucket\n",
        "RAW_PREFIX = \"raw/\"                             # folder containing sources\n",
        "TRANSFORMED_PREFIX = \"transformed/\"             # where transformed CSV will be stored\n",
        "LOGS_PREFIX = \"logs/\"\n",
        "\n",
        "RDS_HOST = \"bhuguvidb.cd6ku6emavna.ap-southeast-2.rds.amazonaws.com\"\n",
        "RDS_PORT = 3306\n",
        "RDS_USER = \"admin\"\n",
        "RDS_PASSWORD = \"Projectmukkiyam\"              # <<< REPLACE locally in Colab\n",
        "RDS_DB = \"bhuguvidb\"\n",
        "TARGET_TABLE = \"etl_merged_data\"\n",
        "\n",
        "# local reference to uploaded file (from your session)\n",
        "LOCAL_UPLOADED_FILE = \"/mnt/data/df29f879-84a3-47a7-9da2-536545724c92.png\"\n",
        "\n",
        "# -------------------------\n",
        "# Logging (ensure flushed before upload)\n",
        "# -------------------------\n",
        "LOG_FILE = \"etl_run.log\"\n",
        "\n",
        "# clear existing handlers\n",
        "for h in logging.root.handlers[:]:\n",
        "    logging.root.removeHandler(h)\n",
        "\n",
        "logging.basicConfig(\n",
        "    filename=LOG_FILE,\n",
        "    filemode=\"w\",\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger()\n",
        "logger.addHandler(logging.StreamHandler())\n",
        "\n",
        "logger.info(\"ETL job starting\")\n",
        "\n",
        "# -------------------------\n",
        "# boto3 session & S3 client\n",
        "# -------------------------\n",
        "if AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY:\n",
        "    session = boto3.Session(\n",
        "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "        region_name=AWS_REGION\n",
        "    )\n",
        "else:\n",
        "    session = boto3.Session(region_name=AWS_REGION)\n",
        "\n",
        "s3 = session.client(\"s3\")\n",
        "\n",
        "# quick S3 access check\n",
        "try:\n",
        "    s3.head_bucket(Bucket=S3_BUCKET)\n",
        "    logger.info(\"Connected to S3 bucket: %s\", S3_BUCKET)\n",
        "except botocore.exceptions.ClientError as e:\n",
        "    logger.exception(\"S3 access failed: %s\", e)\n",
        "    raise SystemExit(\"S3 access failed - check bucket name and credentials\")\n",
        "\n",
        "# -------------------------\n",
        "# Helpers: list, extract, parse\n",
        "# -------------------------\n",
        "def list_s3_objects(bucket, prefix):\n",
        "    logger.info(\"Listing objects under s3://%s/%s\", bucket, prefix)\n",
        "    keys = []\n",
        "    paginator = s3.get_paginator(\"list_objects_v2\")\n",
        "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            k = obj[\"Key\"]\n",
        "            if not k.endswith(\"/\"):\n",
        "                keys.append(k)\n",
        "    logger.info(\"Found %d object(s) under prefix\", len(keys))\n",
        "    return keys\n",
        "\n",
        "def extract_csv(bucket, key):\n",
        "    logger.info(\"Extracting CSV: %s\", key)\n",
        "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
        "    return pd.read_csv(BytesIO(resp[\"Body\"].read()))\n",
        "\n",
        "def extract_jsonlines(bucket, key):\n",
        "    logger.info(\"Extracting JSON (lines or array): %s\", key)\n",
        "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
        "    txt = resp[\"Body\"].read().decode()\n",
        "    # try json-lines then fallback to normal json array\n",
        "    try:\n",
        "        return pd.read_json(StringIO(txt), lines=True)\n",
        "    except ValueError:\n",
        "        return pd.read_json(StringIO(txt))\n",
        "\n",
        "def extract_xml(bucket, key, item_tag=\"person\"):\n",
        "    logger.info(\"Extracting XML: %s\", key)\n",
        "    resp = s3.get_object(Bucket=bucket, Key=key)\n",
        "    txt = resp[\"Body\"].read().decode()\n",
        "    root = ET.fromstring(txt)\n",
        "    rows = []\n",
        "    # first try to find repeated item_tag elements\n",
        "    for elem in root.findall('.//{}'.format(item_tag)):\n",
        "        row = {child.tag: child.text for child in elem}\n",
        "        rows.append(row)\n",
        "    # fallback: if no item_tag matches, try each immediate child of root\n",
        "    if not rows:\n",
        "        for elem in root:\n",
        "            row = {child.tag: child.text for child in elem}\n",
        "            rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# -------------------------\n",
        "# Normalize & Transform\n",
        "# -------------------------\n",
        "def normalize_and_transform(df):\n",
        "    logger.info(\"Normalizing columns and transforming units\")\n",
        "    # normalize column names\n",
        "    df = df.copy()\n",
        "    df.columns = [c.strip().lower() for c in df.columns]\n",
        "\n",
        "    # keep only relevant columns if present\n",
        "    # ensure name exists\n",
        "    if \"name\" not in df.columns:\n",
        "        logger.warning(\"'name' not found; creating synthetic names\")\n",
        "        df[\"name\"] = [f\"name_{i}\" for i in range(len(df))]\n",
        "\n",
        "    # numeric conversion\n",
        "    for col in [\"height\", \"weight\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "    # convert inches -> meters and pounds -> kg (if present)\n",
        "    if \"height\" in df.columns:\n",
        "        df[\"height_m\"] = df[\"height\"] * 0.0254\n",
        "    else:\n",
        "        df[\"height_m\"] = None\n",
        "\n",
        "    if \"weight\" in df.columns:\n",
        "        df[\"weight_kg\"] = df[\"weight\"] * 0.453592\n",
        "    else:\n",
        "        df[\"weight_kg\"] = None\n",
        "\n",
        "    out = df[[\"name\", \"height_m\", \"weight_kg\"]].reset_index(drop=True)\n",
        "    logger.info(\"Transformation result rows: %d\", len(out))\n",
        "    return out\n",
        "\n",
        "# -------------------------\n",
        "# Save transformed CSV to S3\n",
        "# -------------------------\n",
        "def upload_transformed(df, bucket, key):\n",
        "    logger.info(\"Uploading transformed CSV to s3://%s/%s\", bucket, key)\n",
        "    csv_buf = df.to_csv(index=False)\n",
        "    s3.put_object(Bucket=bucket, Key=key, Body=csv_buf, ContentType=\"text/csv\")\n",
        "    logger.info(\"Uploaded transformed CSV\")\n",
        "\n",
        "# -------------------------\n",
        "# Load to MySQL RDS\n",
        "# -------------------------\n",
        "def ensure_db_exists(user, pwd, host, port, db):\n",
        "    logger.info(\"Ensuring database exists (attempt)\")\n",
        "    try:\n",
        "        engine_tmp = create_engine(f\"mysql+pymysql://{user}:{pwd}@{host}:{port}/\")\n",
        "        conn_tmp = engine_tmp.connect()\n",
        "        conn_tmp.execute(f\"CREATE DATABASE IF NOT EXISTS `{db}`;\")\n",
        "        conn_tmp.close()\n",
        "        logger.info(\"Database exists/created: %s\", db)\n",
        "    except Exception as e:\n",
        "        logger.warning(\"Could not create DB (may lack privileges): %s\", e)\n",
        "\n",
        "def load_to_rds(df, user, pwd, host, port, db, table_name=\"etl_merged_data\"):\n",
        "    logger.info(\"Loading dataframe to RDS table %s.%s\", db, table_name)\n",
        "    # Build engine (SQLAlchemy)\n",
        "    engine = create_engine(f\"mysql+pymysql://{user}:{pwd}@{host}:{port}/{db}\")\n",
        "    try:\n",
        "        df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
        "        logger.info(\"Pushed dataframe to RDS table %s.%s\", db, table_name)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Failed to push to RDS: %s\", e)\n",
        "        raise\n",
        "\n",
        "# -------------------------\n",
        "# Upload log to S3 (flush first)\n",
        "# -------------------------\n",
        "def upload_log_to_s3(bucket, prefix, local_log):\n",
        "    timestamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    key = os.path.join(prefix.rstrip(\"/\"), f\"etl_run_{timestamp}.log\")\n",
        "    logger.info(\"Flushing log handlers before upload\")\n",
        "    for h in logger.handlers:\n",
        "        try:\n",
        "            h.flush()\n",
        "        except Exception:\n",
        "            pass\n",
        "    logger.info(\"Uploading log to s3://%s/%s\", bucket, key)\n",
        "    s3.upload_file(local_log, bucket, key)\n",
        "    logger.info(\"Uploaded log\")\n",
        "\n",
        "# -------------------------\n",
        "# Main pipeline\n",
        "# -------------------------\n",
        "def run_pipeline():\n",
        "    logger.info(\"Pipeline started\")\n",
        "    keys = list_s3_objects(S3_BUCKET, RAW_PREFIX)\n",
        "    if not keys:\n",
        "        logger.error(\"No files found under s3://%s/%s. Exiting.\", S3_BUCKET, RAW_PREFIX)\n",
        "        return\n",
        "\n",
        "    dfs = []\n",
        "    for key in keys:\n",
        "        k = key.lower()\n",
        "        try:\n",
        "            if k.endswith(\".csv\"):\n",
        "                df = extract_csv(S3_BUCKET, key)\n",
        "            elif k.endswith(\".json\"):\n",
        "                df = extract_jsonlines(S3_BUCKET, key)\n",
        "            elif k.endswith(\".xml\"):\n",
        "                df = extract_xml(S3_BUCKET, key, item_tag=\"person\")\n",
        "            else:\n",
        "                logger.warning(\"Skipping unsupported file: %s\", key)\n",
        "                continue\n",
        "            logger.info(\"Extracted %s (rows=%d)\", key, len(df))\n",
        "            df[\"_source_s3_key\"] = key\n",
        "            dfs.append(df)\n",
        "        except Exception as e:\n",
        "            logger.exception(\"Failed to extract %s: %s\", key, e)\n",
        "\n",
        "    if not dfs:\n",
        "        logger.error(\"No data extracted successfully. Exiting.\")\n",
        "        return\n",
        "\n",
        "    combined = pd.concat(dfs, ignore_index=True, sort=False)\n",
        "    logger.info(\"Combined dataframe rows: %d\", len(combined))\n",
        "\n",
        "    transformed = normalize_and_transform(combined)\n",
        "\n",
        "    # Save locally then upload transformed CSV\n",
        "    transformed.to_csv(\"transformed_data.csv\", index=False)\n",
        "    upload_transformed(transformed, S3_BUCKET, os.path.join(TRANSFORMED_PREFIX.rstrip(\"/\"), \"transformed_data.csv\"))\n",
        "\n",
        "    # Ensure DB exists (best-effort)\n",
        "    ensure_db_exists(RDS_USER, RDS_PASSWORD, RDS_HOST, RDS_PORT, RDS_DB)\n",
        "\n",
        "    # Load into RDS\n",
        "    try:\n",
        "        load_to_rds(transformed, RDS_USER, RDS_PASSWORD, RDS_HOST, RDS_PORT, RDS_DB, TARGET_TABLE)\n",
        "    except Exception as e:\n",
        "        logger.exception(\"Loading to RDS failed: %s\", e)\n",
        "        # proceed to upload logs and exit with failure\n",
        "        upload_log_to_s3(S3_BUCKET, LOGS_PREFIX, LOG_FILE)\n",
        "        raise\n",
        "\n",
        "    # Upload logs\n",
        "    upload_log_to_s3(S3_BUCKET, LOGS_PREFIX, LOG_FILE)\n",
        "\n",
        "    logger.info(\"Pipeline completed successfully\")\n",
        "    print(\"Transformed CSV: s3://%s/%s\" % (S3_BUCKET, os.path.join(TRANSFORMED_PREFIX.rstrip(\"/\"), \"transformed_data.csv\")))\n",
        "    print(\"RDS table: %s.%s\" % (RDS_DB, TARGET_TABLE))\n",
        "\n",
        "# Execute pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Readme file"
      ],
      "metadata": {
        "id": "B-LZJtZJdkQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Introduction\n",
        "\n",
        "This project implements a production-grade ETL (Extract ‚Üí Transform ‚Üí Load) pipeline using AWS cloud services.\n",
        "The workflow extracts CSV, JSON, and XML data, performs data transformation, and loads the transformed output into:\n",
        "\n",
        "AWS S3 (raw + transformed zones)\n",
        "\n",
        "AWS RDS (MySQL/PostgreSQL table)\n",
        "\n",
        "The pipeline includes secure credential handling, automatic multi-file processing, and centralized logging stored in S3.\n",
        "\n",
        "This project simulates a real-world data engineering scenario and demonstrates cloud-native ETL execution.\n",
        "\n",
        "üéØ 2. Objectives\n",
        "\n",
        "By completing this project, you achieve the following:\n",
        "\n",
        "‚úî Extract Data\n",
        "\n",
        "Processes multiple file formats: CSV, JSON, XML\n",
        "\n",
        "Reads all files dynamically from S3 under /raw/ prefix\n",
        "\n",
        "‚úî Transform Data\n",
        "\n",
        "Data cleaning\n",
        "\n",
        "Standardizes schema\n",
        "\n",
        "Converts:\n",
        "\n",
        "Height: inches ‚Üí meters\n",
        "\n",
        "Weight: pounds ‚Üí kilograms\n",
        "\n",
        "Combines 9 input files into 1 unified dataset\n",
        "\n",
        "‚úî Load Data\n",
        "\n",
        "Uploads transformed CSV to S3 (/transformed/)\n",
        "\n",
        "Inserts transformed records into AWS RDS table using SQLAlchemy\n",
        "\n",
        "‚úî Logging\n",
        "\n",
        "Every step of ETL is logged\n",
        "\n",
        "Logs stored in S3 (/logs/)\n",
        "\n",
        "‚úî Infrastructure\n",
        "\n",
        "AWS S3 bucket\n",
        "\n",
        "AWS RDS instance\n",
        "\n",
        "Optional AWS Glue for schema inference (not mandatory)\n",
        "\n",
        "üèó 3. Architecture\n",
        "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "            ‚îÇ   Raw Data  ‚îÇ\n",
        "            ‚îÇ CSV/JSON/XML‚îÇ\n",
        "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ\n",
        "                   ‚ñº\n",
        "            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "            ‚îÇ AWS  S3     ‚îÇ\n",
        "            ‚îÇ  raw/       ‚îÇ\n",
        "            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                   ‚îÇ (Extract)\n",
        "                   ‚ñº\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ  ETL Python Job  ‚îÇ\n",
        "          ‚îÇ  (local/EC2)     ‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                 ‚îÇ Transform\n",
        "                 ‚ñº\n",
        "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "     ‚îÇ Cleaned + Converted Data ‚îÇ\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                ‚îÇ Load\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚ñº                      ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ S3 bucket  ‚îÇ        ‚îÇ AWS RDS        ‚îÇ\n",
        "‚îÇ transformed‚îÇ        ‚îÇ etl_merged_data‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "Logs ‚Üí s3://bucket/logs/\n",
        "\n",
        "üß∞ 4. Tools & Technologies Used\n",
        "Tool\tPurpose\n",
        "Python 3\tETL development\n",
        "Boto3\tAWS SDK for S3 interactions\n",
        "Pandas\tData manipulation\n",
        "SQLAlchemy\tRDS database connector\n",
        "AWS S3\tRaw + transformed data lake\n",
        "AWS RDS (MySQL/PostgreSQL)\tPersistent table storage\n",
        "AWS IAM\tSecure access control\n",
        "AWS Glue\tOptional schema inference\n",
        "Logging module\tEnd-to-end ETL logs\n",
        "üîí 5. Secure Coding Practices\n",
        "\n",
        "This project follows secure coding guidelines:\n",
        "\n",
        "No credentials inside code\n",
        "\n",
        "AWS keys stored in:\n",
        "\n",
        "~/.aws/credentials\n",
        "\n",
        "OR environment variables\n",
        "\n",
        "SQL injection prevention using SQLAlchemy ORM\n",
        "\n",
        "PEP-8 compliant code\n",
        "\n",
        "Modular functions for maintainability\n",
        "\n",
        "Clear error handling with logger exceptions\n",
        "\n",
        "üìÅ 6. Project Folder Structure\n",
        "‚îú‚îÄ‚îÄ etl_pipeline.py        # Main modular Python script (single-cell ready)\n",
        "‚îú‚îÄ‚îÄ README.md              # Documentation\n",
        "‚îî‚îÄ‚îÄ assets/\n",
        "      ‚îî‚îÄ‚îÄ screenshots/     # S3, RDS, Architecture\n",
        "\n",
        "üîß 7. Steps to Run the Project\n",
        "Step 1 ‚Äî Clone the GitHub Repository\n",
        "git clone <your-repo-url>\n",
        "cd etl-cloud-project\n",
        "\n",
        "Step 2 ‚Äî Install Dependencies\n",
        "pip install boto3 pandas sqlalchemy pymysql lxml\n",
        "\n",
        "Step 3 ‚Äî Configure AWS Credentials\n",
        "\n",
        "Make sure your AWS credentials are set under:\n",
        "\n",
        "Option A ‚Äî Environment Variables:\n",
        "export AWS_ACCESS_KEY_ID=\"your_key\"\n",
        "export AWS_SECRET_ACCESS_KEY=\"your_secret\"\n",
        "export AWS_DEFAULT_REGION=\"ap-south-1\"\n",
        "\n",
        "Option B ‚Äî AWS CLI:\n",
        "aws configure\n",
        "\n",
        "‚õè 8. Running the ETL Pipeline\n",
        "\n",
        "Run with:\n",
        "\n",
        "python etl_pipeline.py\n",
        "\n",
        "\n",
        "The script performs:\n",
        "\n",
        "Connect to S3\n",
        "\n",
        "Read all files from raw/\n",
        "\n",
        "Extract CSV, JSON, XML\n",
        "\n",
        "Transform dataset\n",
        "\n",
        "Upload combined CSV to /transformed/\n",
        "\n",
        "Insert data into AWS RDS\n",
        "\n",
        "Upload logs to /logs/\n",
        "\n",
        "üß™ 9. Sample Output (Expected Log Snippet)\n",
        "INFO | ETL job starting\n",
        "INFO | Connected to S3 bucket\n",
        "INFO | Extracting CSV: raw/source1.csv\n",
        "INFO | Extracting JSON: raw/source1.json\n",
        "INFO | Extracting XML: raw/source1.xml\n",
        "INFO | Combined dataframe rows: 39\n",
        "INFO | Transformation complete\n",
        "INFO | Uploaded transformed CSV to S3\n",
        "INFO | Loaded 39 rows into RDS table\n",
        "INFO | Logs uploaded to s3://bucket/logs/\n",
        "\n",
        "\n",
        "This is the expected output for successful execution.\n",
        "\n",
        "üìä 10. Final Deliverables\n",
        "\n",
        "You must provide:\n",
        "\n",
        "‚úî Python ETL script\n",
        "‚úî README.md (this file)\n",
        "‚úî GitHub public repository\n",
        "‚úî Project presentation with:\n",
        "\n",
        "Problem statement\n",
        "\n",
        "Architecture\n",
        "\n",
        "Tools used\n",
        "\n",
        "Approach\n",
        "\n",
        "Insights/Challenges\n",
        "\n",
        "Demo screenshots\n",
        "\n",
        "Future improvements"
      ],
      "metadata": {
        "id": "S2Vwh0c_dn2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "\n",
        "conn = pymysql.connect(\n",
        "    host=\"bhuguvidb.cd6ku6emavna.ap-southeast-2.rds.amazonaws.com\",\n",
        "    user=\"admin\",\n",
        "    password=\"Projectmukkiyam\",\n",
        "    port=3306\n",
        ")\n",
        "\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"CREATE DATABASE IF NOT EXISTS bhuguvidb;\")\n",
        "conn.commit()\n",
        "\n",
        "print(\"Database 'bhuguvidb' created successfully!\")\n",
        "conn.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrGU_mN9UtRR",
        "outputId": "c1e30c2c-d40b-4b3d-82de-a2cce31a7651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Database 'bhuguvidb' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "\n",
        "try:\n",
        "    conn = pymysql.connect(\n",
        "        host=\"bhuguvidb.cd6ku6emavna.ap-southeast-2.rds.amazonaws.com\",\n",
        "        user=\"admin\",\n",
        "        password=\"Projectmukkiyam\",\n",
        "        database=\"bhuguvidb\",\n",
        "        port=3306,\n",
        "        connect_timeout=10\n",
        "    )\n",
        "    print(\"SUCCESS ‚Äî Connected to RDS!\")\n",
        "except Exception as e:\n",
        "    print(\"FAILED:\", e)\n"
      ],
      "metadata": {
        "id": "i3gVMqtNVWpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymysql\n",
        "\n",
        "try:\n",
        "    conn = pymysql.connect(\n",
        "        host=\"bhuguvidb.cd6ku6emavna.ap-south-east-2.rds.amazonaws.com\",\n",
        "        user=\"admin\",\n",
        "        password=\"Projectmukkiyam\",\n",
        "        database=\"bhuguvidb\",\n",
        "        port=3306,\n",
        "        connect_timeout=10\n",
        "    )\n",
        "    print(\"SUCCESS ‚Äî Connected to RDS!\")\n",
        "except Exception as e:\n",
        "    print(\"FAILED:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLPCrmqRVBDT",
        "outputId": "0681a65d-2d9c-4884-bb12-77ee9f2e7d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAILED: (2003, \"Can't connect to MySQL server on 'bhuguvidb.cd6ku6emavna.ap-south-east-2.rds.amazonaws.com' ([Errno -2] Name or service not known)\")\n"
          ]
        }
      ]
    }
  ]
}